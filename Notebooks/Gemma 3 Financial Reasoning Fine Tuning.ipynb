{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":282751,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239470,"modelId":222398}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 3 Fine-Tuning: Financial Reasoning Fine-Tuning\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1. Setting Up Working Enviroment ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U datasets \n!pip install -U accelerate \n!pip install -U peft \n!pip install -U trl \n!pip install -U bitsandbytes\n!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:46:58.503759Z","iopub.execute_input":"2025-05-03T12:46:58.504005Z","iopub.status.idle":"2025-05-03T12:49:25.639077Z","shell.execute_reply.started":"2025-05-03T12:46:58.503982Z","shell.execute_reply":"2025-05-03T12:49:25.638206Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:51:49.664645Z","iopub.execute_input":"2025-05-03T12:51:49.664940Z","iopub.status.idle":"2025-05-03T12:51:50.580220Z","shell.execute_reply.started":"2025-05-03T12:51:49.664912Z","shell.execute_reply":"2025-05-03T12:51:50.579514Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 2. Loading the Model and tokenizers","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, Gemma3ForConditionalGeneration\nimport torch\n\nGEMMA_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    GEMMA_PATH, device_map=\"auto\",attn_implementation='eager'\n).eval()\n\ntokenizer = AutoTokenizer.from_pretrained(GEMMA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:52:26.089873Z","iopub.execute_input":"2025-05-03T12:52:26.090129Z","iopub.status.idle":"2025-05-03T12:54:33.117253Z","shell.execute_reply.started":"2025-05-03T12:52:26.090108Z","shell.execute_reply":"2025-05-03T12:54:33.116676Z"}},"outputs":[{"name":"stderr","text":"2025-05-03 12:52:41.731412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746276762.159263      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746276762.281027      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5125ef6246c1480cae70f882f371cb88"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# 3. Test the Model with Zero-Shot Inference","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"TheFinAI/Fino1_Reasoning_Path_FinQA\", split = \"train[0:500]\",trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:11:28.725725Z","iopub.execute_input":"2025-05-03T13:11:28.726023Z","iopub.status.idle":"2025-05-03T13:11:31.999365Z","shell.execute_reply.started":"2025-05-03T13:11:28.726001Z","shell.execute_reply":"2025-05-03T13:11:31.998834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17788e1e8ba474688eecee96343a4d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/14.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d0a4f047ea94c578d0885ffe87ce610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb1608d4cd9446d9ee933ed2dcec68d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:12:31.070585Z","iopub.execute_input":"2025-05-03T13:12:31.071589Z","iopub.status.idle":"2025-05-03T13:12:31.075280Z","shell.execute_reply.started":"2025-05-03T13:12:31.071563Z","shell.execute_reply":"2025-05-03T13:12:31.074543Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"question = dataset[0]['Open-ended Verifiable Question']\n\n\ninputs = tokenizer(\n    [prompt_style.format(question, \"\") + tokenizer.eos_token],\n    return_tensors=\"pt\"\n).to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    eos_token_id=tokenizer.eos_token_id,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:12:46.043654Z","iopub.execute_input":"2025-05-03T13:12:46.043949Z","iopub.status.idle":"2025-05-03T13:14:46.899356Z","shell.execute_reply.started":"2025-05-03T13:12:46.043929Z","shell.execute_reply":"2025-05-03T13:14:46.898757Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\nThe question asks for the portion of the estimated amortization expense that will be recognized in 2017.\nThe provided table shows the estimated amortization expense for intangible assets for the years 2017, 2018, 2019, 2020, 2021, and 2022 and thereafter.\nThe amortization expense for 2017 is $10,509.\n</think>\n$10,509\n\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 4. Loading & Processing Dataset","metadata":{}},{"cell_type":"code","source":"train_prompt_style=\"\"\"\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:23:58.210455Z","iopub.execute_input":"2025-05-03T13:23:58.211019Z","iopub.status.idle":"2025-05-03T13:23:58.214733Z","shell.execute_reply.started":"2025-05-03T13:23:58.210996Z","shell.execute_reply":"2025-05-03T13:23:58.214161Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    inputs = examples[\"Open-ended Verifiable Question\"]\n    complex_cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for question, cot, response in zip(inputs, complex_cots, outputs):\n        # Append the EOS token to the response if it's not already there\n        if not response.endswith(tokenizer.eos_token):\n            response += tokenizer.eos_token\n        text = train_prompt_style.format(question, cot, response)\n        texts.append(text)\n    return {\"text\": texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:24:00.705704Z","iopub.execute_input":"2025-05-03T13:24:00.705963Z","iopub.status.idle":"2025-05-03T13:24:00.710536Z","shell.execute_reply.started":"2025-05-03T13:24:00.705940Z","shell.execute_reply":"2025-05-03T13:24:00.709937Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"TheFinAI/Fino1_Reasoning_Path_FinQA\", split = \"train[0:500]\",trust_remote_code=True)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:24:06.383040Z","iopub.execute_input":"2025-05-03T13:24:06.383634Z","iopub.status.idle":"2025-05-03T13:24:07.514826Z","shell.execute_reply.started":"2025-05-03T13:24:06.383610Z","shell.execute_reply":"2025-05-03T13:24:07.514129Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5dd5c3e0a9499db58789e9d38550d6"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"\\nBelow is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\nAlright, let's dive into this question about amortization expenses. First, I need to identify the table that illustrates the estimated amortization expenses from 2017 onwards. It looks like this table is super important in figuring out the expense for 2017. \\n\\nOkay, found it! The table clearly states that the estimated amortization expense for the year 2017 is $10,509, but it notes that this is in thousands. \\n\\nLet me think for a second. If the numbers are in thousands, that means I need to convert that $10,509 into actual dollars, which becomes $10,509,000. Easy enough, I just multiply by a thousand. \\n\\nI need to make sure this is consistent throughout. Let me double-check if there's any other information that throws a wrench in this conversion... Nope, everything seems straightforward here. \\n\\nSo, what's the conclusion? The amortization expense for 2017 is a clear-cut $10,509,000. Hmm, that makes sense based on the conversion from thousands of dollars, and it looks like there aren't any surprise twists or exceptions here. \\n\\nBut hold up, let me consider the perspective of portions. The context refers to percentages or parts, and for that, I should be thinking about the total amortization expense. The whole amount listed is $58,370,000 for all the intangible assets. \\n\\nTo find out what part of the total this 2017 expense represents, I'll calculate a ratio. So, I divide $10,509,000 by the grand total, $58,370,000. Crunching these numbers gives me around 0.18004.\\n\\nSo, in conclusion, the portion of the total estimated amortization expense that is specifically for 2017 is approximately 0.18004 of the total. I'm confident that's correct since that ratio aligns perfectly with the expectation of finding a portion or part. \\n\\nPhew, that should do it!\\n</think>\\nTo determine the portion of the estimated amortization expense that will be recognized in 2017, we first examine the table provided. The estimated amortization expense for 2017 is $10,509,000 (noting that the figures are in thousands, hence $10,509 becomes $10,509,000). \\n\\nThe total estimated amortization expense for all the listed years is $58,370,000. To find out what portion of the total this represents for 2017, we take the 2017 expense and divide it by the total:\\n\\n\\\\[ \\\\text{Portion for 2017} = \\\\frac{\\\\text{2017 expense}}{\\\\text{Total expense}} = \\\\frac{10,509,000}{58,370,000} \\\\approx 0.18004 \\\\]\\n\\nTherefore, approximately 18.004% of the total estimated amortization expense will be recognized in 2017.<eos>\\n\""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # we're doing causal LM, not masked LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:25:47.867255Z","iopub.execute_input":"2025-05-03T13:25:47.867942Z","iopub.status.idle":"2025-05-03T13:25:47.883383Z","shell.execute_reply.started":"2025-05-03T13:25:47.867917Z","shell.execute_reply":"2025-05-03T13:25:47.882716Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 5. Setting up the Model Training Pipeline","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom peft import LoraConfig\n\n# LoRA Configuration\npeft_config = LoraConfig(\n    lora_alpha=16,                           # Scaling factor for LoRA\n    lora_dropout=0.05,                       # Add slight dropout for regularization\n    r=64,                                    # Rank of the LoRA update matrices\n    bias=\"none\",                             # No bias reparameterization\n    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],  # Target modules for LoRA\n)\n\n\n# Training Arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"output\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    logging_steps=0.2,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"none\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:27:31.799057Z","iopub.execute_input":"2025-05-03T13:27:31.799542Z","iopub.status.idle":"2025-05-03T13:27:33.618432Z","shell.execute_reply.started":"2025-05-03T13:27:31.799520Z","shell.execute_reply":"2025-05-03T13:27:33.617706Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:28:27.630001Z","iopub.execute_input":"2025-05-03T13:28:27.630717Z","iopub.status.idle":"2025-05-03T13:28:34.001100Z","shell.execute_reply.started":"2025-05-03T13:28:27.630692Z","shell.execute_reply":"2025-05-03T13:28:34.000372Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b373c4f4a94c4889c77aeac52d2a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0457d0168f2b4db89e03ccc3596e0390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b32f638b94248cf8d3426407486ea7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72823780775d4ceb9af15a7b5c647396"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 6. Model Fine-Tuning with LoRA","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:28:38.602621Z","iopub.execute_input":"2025-05-03T13:28:38.602922Z","iopub.status.idle":"2025-05-03T14:45:39.326545Z","shell.execute_reply.started":"2025-05-03T13:28:38.602901Z","shell.execute_reply":"2025-05-03T14:45:39.325911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 1:16:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>3.543400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.803400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.719600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.604300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.627300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# 7. Model Inference After Fine-Tuning","metadata":{}},{"cell_type":"code","source":"question = dataset[0]['Open-ended Verifiable Question']\n\ninputs = tokenizer(\n    [prompt_style.format(question, \"\") + tokenizer.eos_token],\n    return_tensors=\"pt\"\n).to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    eos_token_id=tokenizer.eos_token_id,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:45:39.327770Z","iopub.execute_input":"2025-05-03T14:45:39.328378Z","iopub.status.idle":"2025-05-03T14:46:40.790135Z","shell.execute_reply.started":"2025-05-03T14:45:39.328358Z","shell.execute_reply":"2025-05-03T14:46:40.789529Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n\nAlright, let's figure out what portion of the estimated amortization expense will be recognized in 2017. First, I need to see what the total estimated amortization expense is for the years 2017 through 2022. Looking at the table, the total estimated amortization expense for these years is $58,370.\n\nNow, I need to find out what portion of this total is specifically for 2017. The table shows that the estimated amortization expense for 2017 is $10,509.\n\nTo find out what percentage of the total expense this is, I'll divide the 2017 expense by the total expense and then multiply by 100 to get a percentage. So, I'll do this: $10,509 divided by $58,370, and then multiply by 100.\n\nLet's do the math. First, I'll divide $10,509 by $58,370. This gives me approximately 0.1802.\n\nNext, I'll multiply this result by 100 to convert it into a percentage. So, 0.1802 times 100 gives me 18.02%.\n\nTherefore, the estimated amortization expense for 2017 is about 18.02% of the total estimated amortization expense for the years 2017 through 2022.\n\n</think>\nThe estimated amortization expense for 2017 is $10,509. This represents approximately 18.02% of the total estimated amortization expense for the years 2017 through 2022, which is $58,370.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# 8. Saving the Model and Tokenizer to Hugging Face","metadata":{}},{"cell_type":"code","source":"new_model_online = \"youssefhosni/Gemma-3-4B-Financial-Reasoning\"\nnew_model_local = \"Gemma-3-4B-Fin-QA-Reasoning\"\nmodel.save_pretrained(new_model_local) # Local saving\ntokenizer.save_pretrained(new_model_local)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(new_model_online) # Online saving\ntokenizer.push_to_hub(new_model_online) # Online saving","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}